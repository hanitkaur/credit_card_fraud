{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4q1tB5Uf6PB"
      },
      "source": [
        "## Problem Statement :\n",
        "In the banking industry, detecting credit card fraud using machine learning is not just a trend; it is a necessity for banks, as they need to put proactive monitoring and fraud prevention mechanisms in place. Machine learning helps these institutions reduce time-consuming manual reviews, costly chargebacks and fees, and denial of legitimate transactions.\n",
        "\n",
        "Suppose you are part of the analytics team working on a fraud detection model and its cost-benefit analysis. You need to develop a machine learning model to detect fraudulent transactions based on the historical transactional data of customers with a pool of merchants. You can learn more about transactional data and the creation of historical variables from the link attached here. You may find this helpful in the capstone project while building the fraud detection model. Based on your understanding of the model, you have to analyse the business impact of these fraudulent transactions and recommend the optimal ways that the bank can adopt to mitigate the fraud risks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w9CvyO5EMPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f735dc98-b1a2-44e2-b8d0-9496446edffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4MIcCJyF2FW"
      },
      "outputs": [],
      "source": [
        "#Importing Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "%matplotlib inline\n",
        "from scipy.stats import randint\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix, accuracy_score\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('float_format','{:.2f}'. format)\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "7oPO-YuFFLt0",
        "outputId": "8e9f4803-7cb9-4930-f6f9-5bef915b1794"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
              "0           0   2019-01-01 00:00:18  2703186189652095   \n",
              "1           1   2019-01-01 00:00:44      630423337322   \n",
              "2           2   2019-01-01 00:00:51    38859492057661   \n",
              "3           3   2019-01-01 00:01:16  3534093764340240   \n",
              "\n",
              "                             merchant       category    amt      first  \\\n",
              "0          fraud_Rippin, Kub and Mann       misc_net   4.97   Jennifer   \n",
              "1     fraud_Heller, Gutmann and Zieme    grocery_pos 107.23  Stephanie   \n",
              "2                fraud_Lind-Buckridge  entertainment 220.11     Edward   \n",
              "3  fraud_Kutch, Hermiston and Farrell  gas_transport  45.00     Jeremy   \n",
              "\n",
              "      last gender                        street            city state  \\\n",
              "0    Banks      F                561 Perry Cove  Moravian Falls    NC   \n",
              "1     Gill      F  43039 Riley Greens Suite 393          Orient    WA   \n",
              "2  Sanchez      M      594 White Dale Suite 530      Malad City    ID   \n",
              "3    White      M   9443 Cynthia Court Apt. 038         Boulder    MT   \n",
              "\n",
              "       zip   lat    long  city_pop                                job  \\\n",
              "0 28654.00 36.08  -81.18   3495.00          Psychologist, counselling   \n",
              "1 99160.00 48.89 -118.21    149.00  Special educational needs teacher   \n",
              "2 83252.00 42.18 -112.26   4154.00        Nature conservation officer   \n",
              "3 59632.00 46.23 -112.11   1939.00                    Patent attorney   \n",
              "\n",
              "          dob                         trans_num     unix_time  merch_lat  \\\n",
              "0  1988-03-09  0b242abb623afc578575680df30655b9 1325376018.00      36.01   \n",
              "1  1978-06-21  1f76529f8574734946361c461b024d99 1325376044.00      49.16   \n",
              "2  1962-01-19  a1a22d70485983eac12b5b88dad1cf95 1325376051.00      43.15   \n",
              "3  1967-01-12  6b849c168bdad6f867558c3793159a81 1325376076.00      47.03   \n",
              "\n",
              "   merch_long  is_fraud  \n",
              "0      -82.05      0.00  \n",
              "1     -118.19      0.00  \n",
              "2     -112.15      0.00  \n",
              "3     -112.56      0.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e183b10-399c-4111-892f-45e4a6e5f642\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>trans_date_trans_time</th>\n",
              "      <th>cc_num</th>\n",
              "      <th>merchant</th>\n",
              "      <th>category</th>\n",
              "      <th>amt</th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>gender</th>\n",
              "      <th>street</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>zip</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>city_pop</th>\n",
              "      <th>job</th>\n",
              "      <th>dob</th>\n",
              "      <th>trans_num</th>\n",
              "      <th>unix_time</th>\n",
              "      <th>merch_lat</th>\n",
              "      <th>merch_long</th>\n",
              "      <th>is_fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2019-01-01 00:00:18</td>\n",
              "      <td>2703186189652095</td>\n",
              "      <td>fraud_Rippin, Kub and Mann</td>\n",
              "      <td>misc_net</td>\n",
              "      <td>4.97</td>\n",
              "      <td>Jennifer</td>\n",
              "      <td>Banks</td>\n",
              "      <td>F</td>\n",
              "      <td>561 Perry Cove</td>\n",
              "      <td>Moravian Falls</td>\n",
              "      <td>NC</td>\n",
              "      <td>28654.00</td>\n",
              "      <td>36.08</td>\n",
              "      <td>-81.18</td>\n",
              "      <td>3495.00</td>\n",
              "      <td>Psychologist, counselling</td>\n",
              "      <td>1988-03-09</td>\n",
              "      <td>0b242abb623afc578575680df30655b9</td>\n",
              "      <td>1325376018.00</td>\n",
              "      <td>36.01</td>\n",
              "      <td>-82.05</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-01 00:00:44</td>\n",
              "      <td>630423337322</td>\n",
              "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
              "      <td>grocery_pos</td>\n",
              "      <td>107.23</td>\n",
              "      <td>Stephanie</td>\n",
              "      <td>Gill</td>\n",
              "      <td>F</td>\n",
              "      <td>43039 Riley Greens Suite 393</td>\n",
              "      <td>Orient</td>\n",
              "      <td>WA</td>\n",
              "      <td>99160.00</td>\n",
              "      <td>48.89</td>\n",
              "      <td>-118.21</td>\n",
              "      <td>149.00</td>\n",
              "      <td>Special educational needs teacher</td>\n",
              "      <td>1978-06-21</td>\n",
              "      <td>1f76529f8574734946361c461b024d99</td>\n",
              "      <td>1325376044.00</td>\n",
              "      <td>49.16</td>\n",
              "      <td>-118.19</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2019-01-01 00:00:51</td>\n",
              "      <td>38859492057661</td>\n",
              "      <td>fraud_Lind-Buckridge</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>220.11</td>\n",
              "      <td>Edward</td>\n",
              "      <td>Sanchez</td>\n",
              "      <td>M</td>\n",
              "      <td>594 White Dale Suite 530</td>\n",
              "      <td>Malad City</td>\n",
              "      <td>ID</td>\n",
              "      <td>83252.00</td>\n",
              "      <td>42.18</td>\n",
              "      <td>-112.26</td>\n",
              "      <td>4154.00</td>\n",
              "      <td>Nature conservation officer</td>\n",
              "      <td>1962-01-19</td>\n",
              "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
              "      <td>1325376051.00</td>\n",
              "      <td>43.15</td>\n",
              "      <td>-112.15</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-01 00:01:16</td>\n",
              "      <td>3534093764340240</td>\n",
              "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
              "      <td>gas_transport</td>\n",
              "      <td>45.00</td>\n",
              "      <td>Jeremy</td>\n",
              "      <td>White</td>\n",
              "      <td>M</td>\n",
              "      <td>9443 Cynthia Court Apt. 038</td>\n",
              "      <td>Boulder</td>\n",
              "      <td>MT</td>\n",
              "      <td>59632.00</td>\n",
              "      <td>46.23</td>\n",
              "      <td>-112.11</td>\n",
              "      <td>1939.00</td>\n",
              "      <td>Patent attorney</td>\n",
              "      <td>1967-01-12</td>\n",
              "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
              "      <td>1325376076.00</td>\n",
              "      <td>47.03</td>\n",
              "      <td>-112.56</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e183b10-399c-4111-892f-45e4a6e5f642')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5e183b10-399c-4111-892f-45e4a6e5f642 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5e183b10-399c-4111-892f-45e4a6e5f642');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df_fraud_train=pd.read_csv('fraudTrain.csv')\n",
        "df_fraud_train.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fraud_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtDnuN-VLUoG",
        "outputId": "ad16c4ed-1916-4b51-df01-d4367a6e2e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31151 entries, 0 to 31150\n",
            "Data columns (total 23 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   Unnamed: 0             31151 non-null  int64  \n",
            " 1   trans_date_trans_time  31151 non-null  object \n",
            " 2   cc_num                 31151 non-null  int64  \n",
            " 3   merchant               31151 non-null  object \n",
            " 4   category               31151 non-null  object \n",
            " 5   amt                    31151 non-null  float64\n",
            " 6   first                  31151 non-null  object \n",
            " 7   last                   31151 non-null  object \n",
            " 8   gender                 31151 non-null  object \n",
            " 9   street                 31151 non-null  object \n",
            " 10  city                   31150 non-null  object \n",
            " 11  state                  31150 non-null  object \n",
            " 12  zip                    31150 non-null  float64\n",
            " 13  lat                    31150 non-null  float64\n",
            " 14  long                   31150 non-null  float64\n",
            " 15  city_pop               31150 non-null  float64\n",
            " 16  job                    31150 non-null  object \n",
            " 17  dob                    31150 non-null  object \n",
            " 18  trans_num              31150 non-null  object \n",
            " 19  unix_time              31150 non-null  float64\n",
            " 20  merch_lat              31150 non-null  float64\n",
            " 21  merch_long             31150 non-null  float64\n",
            " 22  is_fraud               31150 non-null  float64\n",
            "dtypes: float64(9), int64(2), object(12)\n",
            "memory usage: 5.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtIDsAfWJv3b"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "AG73Et8RJUIh",
        "outputId": "37b288ba-c0d2-42f7-b162-9195daa31642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  trans_date_trans_time            cc_num                         merchant  \\\n",
              "0   2019-01-01 00:00:18  2703186189652095       fraud_Rippin, Kub and Mann   \n",
              "1   2019-01-01 00:00:44      630423337322  fraud_Heller, Gutmann and Zieme   \n",
              "2   2019-01-01 00:00:51    38859492057661             fraud_Lind-Buckridge   \n",
              "\n",
              "        category    amt      first     last gender  \\\n",
              "0       misc_net   4.97   Jennifer    Banks      F   \n",
              "1    grocery_pos 107.23  Stephanie     Gill      F   \n",
              "2  entertainment 220.11     Edward  Sanchez      M   \n",
              "\n",
              "                         street            city state      zip   lat    long  \\\n",
              "0                561 Perry Cove  Moravian Falls    NC 28654.00 36.08  -81.18   \n",
              "1  43039 Riley Greens Suite 393          Orient    WA 99160.00 48.89 -118.21   \n",
              "2      594 White Dale Suite 530      Malad City    ID 83252.00 42.18 -112.26   \n",
              "\n",
              "   city_pop                                job         dob  \\\n",
              "0   3495.00          Psychologist, counselling  1988-03-09   \n",
              "1    149.00  Special educational needs teacher  1978-06-21   \n",
              "2   4154.00        Nature conservation officer  1962-01-19   \n",
              "\n",
              "                          trans_num     unix_time  merch_lat  merch_long  \\\n",
              "0  0b242abb623afc578575680df30655b9 1325376018.00      36.01      -82.05   \n",
              "1  1f76529f8574734946361c461b024d99 1325376044.00      49.16     -118.19   \n",
              "2  a1a22d70485983eac12b5b88dad1cf95 1325376051.00      43.15     -112.15   \n",
              "\n",
              "   is_fraud  \n",
              "0      0.00  \n",
              "1      0.00  \n",
              "2      0.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df2a3bbf-c345-4cc2-90d9-a1efbe229628\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trans_date_trans_time</th>\n",
              "      <th>cc_num</th>\n",
              "      <th>merchant</th>\n",
              "      <th>category</th>\n",
              "      <th>amt</th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>gender</th>\n",
              "      <th>street</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>zip</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>city_pop</th>\n",
              "      <th>job</th>\n",
              "      <th>dob</th>\n",
              "      <th>trans_num</th>\n",
              "      <th>unix_time</th>\n",
              "      <th>merch_lat</th>\n",
              "      <th>merch_long</th>\n",
              "      <th>is_fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-01 00:00:18</td>\n",
              "      <td>2703186189652095</td>\n",
              "      <td>fraud_Rippin, Kub and Mann</td>\n",
              "      <td>misc_net</td>\n",
              "      <td>4.97</td>\n",
              "      <td>Jennifer</td>\n",
              "      <td>Banks</td>\n",
              "      <td>F</td>\n",
              "      <td>561 Perry Cove</td>\n",
              "      <td>Moravian Falls</td>\n",
              "      <td>NC</td>\n",
              "      <td>28654.00</td>\n",
              "      <td>36.08</td>\n",
              "      <td>-81.18</td>\n",
              "      <td>3495.00</td>\n",
              "      <td>Psychologist, counselling</td>\n",
              "      <td>1988-03-09</td>\n",
              "      <td>0b242abb623afc578575680df30655b9</td>\n",
              "      <td>1325376018.00</td>\n",
              "      <td>36.01</td>\n",
              "      <td>-82.05</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-01 00:00:44</td>\n",
              "      <td>630423337322</td>\n",
              "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
              "      <td>grocery_pos</td>\n",
              "      <td>107.23</td>\n",
              "      <td>Stephanie</td>\n",
              "      <td>Gill</td>\n",
              "      <td>F</td>\n",
              "      <td>43039 Riley Greens Suite 393</td>\n",
              "      <td>Orient</td>\n",
              "      <td>WA</td>\n",
              "      <td>99160.00</td>\n",
              "      <td>48.89</td>\n",
              "      <td>-118.21</td>\n",
              "      <td>149.00</td>\n",
              "      <td>Special educational needs teacher</td>\n",
              "      <td>1978-06-21</td>\n",
              "      <td>1f76529f8574734946361c461b024d99</td>\n",
              "      <td>1325376044.00</td>\n",
              "      <td>49.16</td>\n",
              "      <td>-118.19</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-01 00:00:51</td>\n",
              "      <td>38859492057661</td>\n",
              "      <td>fraud_Lind-Buckridge</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>220.11</td>\n",
              "      <td>Edward</td>\n",
              "      <td>Sanchez</td>\n",
              "      <td>M</td>\n",
              "      <td>594 White Dale Suite 530</td>\n",
              "      <td>Malad City</td>\n",
              "      <td>ID</td>\n",
              "      <td>83252.00</td>\n",
              "      <td>42.18</td>\n",
              "      <td>-112.26</td>\n",
              "      <td>4154.00</td>\n",
              "      <td>Nature conservation officer</td>\n",
              "      <td>1962-01-19</td>\n",
              "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
              "      <td>1325376051.00</td>\n",
              "      <td>43.15</td>\n",
              "      <td>-112.15</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df2a3bbf-c345-4cc2-90d9-a1efbe229628')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df2a3bbf-c345-4cc2-90d9-a1efbe229628 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df2a3bbf-c345-4cc2-90d9-a1efbe229628');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Dropping the first column from train as it is of no use\n",
        "df_fraud_train.drop(df_fraud_train.columns[0],axis=1,inplace=True)\n",
        "\n",
        "#Dropping the first column from test as it is of no use\n",
        "df_fraud_test.drop(df_fraud_test.columns[0],axis=1,inplace=True)\n",
        "\n",
        "#Checking the dataset head\n",
        "df_fraud_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeUYspLQJUOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f549bc8e-0a8e-48a3-8f90-621a0b370c60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((31151, 22), (3901, 22))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#Checking the shape for train and test\n",
        "df_fraud_train.shape, df_fraud_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y-Ev9aKJURX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7253646-ccd3-4603-b7fd-13d369b7cacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31151 entries, 0 to 31150\n",
            "Data columns (total 22 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   trans_date_trans_time  31151 non-null  object \n",
            " 1   cc_num                 31151 non-null  int64  \n",
            " 2   merchant               31151 non-null  object \n",
            " 3   category               31151 non-null  object \n",
            " 4   amt                    31151 non-null  float64\n",
            " 5   first                  31151 non-null  object \n",
            " 6   last                   31151 non-null  object \n",
            " 7   gender                 31151 non-null  object \n",
            " 8   street                 31151 non-null  object \n",
            " 9   city                   31150 non-null  object \n",
            " 10  state                  31150 non-null  object \n",
            " 11  zip                    31150 non-null  float64\n",
            " 12  lat                    31150 non-null  float64\n",
            " 13  long                   31150 non-null  float64\n",
            " 14  city_pop               31150 non-null  float64\n",
            " 15  job                    31150 non-null  object \n",
            " 16  dob                    31150 non-null  object \n",
            " 17  trans_num              31150 non-null  object \n",
            " 18  unix_time              31150 non-null  float64\n",
            " 19  merch_lat              31150 non-null  float64\n",
            " 20  merch_long             31150 non-null  float64\n",
            " 21  is_fraud               31150 non-null  float64\n",
            "dtypes: float64(9), int64(1), object(12)\n",
            "memory usage: 5.2+ MB\n"
          ]
        }
      ],
      "source": [
        "#Now we will check the dat types for train data\n",
        "df_fraud_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKazD8OoJUUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a450dacd-8501-42a4-b269-09a85189fa6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3901 entries, 0 to 3900\n",
            "Data columns (total 22 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   trans_date_trans_time  3901 non-null   object \n",
            " 1   cc_num                 3901 non-null   int64  \n",
            " 2   merchant               3901 non-null   object \n",
            " 3   category               3901 non-null   object \n",
            " 4   amt                    3901 non-null   float64\n",
            " 5   first                  3901 non-null   object \n",
            " 6   last                   3901 non-null   object \n",
            " 7   gender                 3901 non-null   object \n",
            " 8   street                 3901 non-null   object \n",
            " 9   city                   3901 non-null   object \n",
            " 10  state                  3901 non-null   object \n",
            " 11  zip                    3901 non-null   int64  \n",
            " 12  lat                    3901 non-null   float64\n",
            " 13  long                   3901 non-null   object \n",
            " 14  city_pop               3900 non-null   float64\n",
            " 15  job                    3900 non-null   object \n",
            " 16  dob                    3900 non-null   object \n",
            " 17  trans_num              3900 non-null   object \n",
            " 18  unix_time              3900 non-null   float64\n",
            " 19  merch_lat              3900 non-null   float64\n",
            " 20  merch_long             3900 non-null   float64\n",
            " 21  is_fraud               3900 non-null   float64\n",
            "dtypes: float64(7), int64(2), object(13)\n",
            "memory usage: 670.6+ KB\n"
          ]
        }
      ],
      "source": [
        "#Now we will check the dat types for test data\n",
        "df_fraud_test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyjmvpzBMUF4"
      },
      "source": [
        "- As we can see trans_date_trans_time and dob col dtype is in object instead data time format. We will change it date time format.\n",
        "- Unix time we can see that it is in int we need to change it time stamp\n",
        "- Also from above we can see that there are no null values present in the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K40wFyyZJUZL"
      },
      "outputs": [],
      "source": [
        "#Converting the trans_date_trans_time and dob into date time format in train and test data\n",
        "df_fraud_train['trans_date_trans_time']=pd.to_datetime(df_fraud_train['trans_date_trans_time'])\n",
        "df_fraud_train['dob']=pd.to_datetime(df_fraud_train['dob'])\n",
        "\n",
        "df_fraud_test['trans_date_trans_time']=pd.to_datetime(df_fraud_test['trans_date_trans_time'])\n",
        "df_fraud_test['dob']=pd.to_datetime(df_fraud_test['dob'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHfr-l9tNBl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "21c66c02-cd83-4a58-dce3-82fbffc58a26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ef9bebe270e5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating new column for unix time stamp and Converting unix time into time stamp for further analysis into train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_fraud_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unix_Time_Stamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_fraud_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unix_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_fraud_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unix_Time_Stamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_fraud_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unix_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid value NaN (not a number)"
          ]
        }
      ],
      "source": [
        "# Creating new column for unix time stamp and Converting unix time into time stamp for further analysis into train and test data\n",
        "\n",
        "df_fraud_train['Unix_Time_Stamp']=df_fraud_train['unix_time'].apply(datetime.utcfromtimestamp)\n",
        "\n",
        "df_fraud_test['Unix_Time_Stamp']=df_fraud_test['unix_time'].apply(datetime.utcfromtimestamp)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dropping unix_time from the train and test data\n",
        "df_fraud_train.drop('unix_time', axis=1, inplace=True)\n",
        "df_fraud_test.drop('unix_time', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk-FuVfhNBoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad88f419-0210-4e4d-e497-02e4d9b3e7af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trans_date_trans_time   0.00\n",
              "cc_num                  0.00\n",
              "merchant                0.00\n",
              "category                0.00\n",
              "amt                     0.00\n",
              "first                   0.00\n",
              "last                    0.00\n",
              "gender                  0.00\n",
              "street                  0.00\n",
              "city                    0.00\n",
              "state                   0.00\n",
              "zip                     0.00\n",
              "lat                     0.00\n",
              "long                    0.00\n",
              "city_pop                0.00\n",
              "job                     0.00\n",
              "dob                     0.00\n",
              "trans_num               0.00\n",
              "unix_time               0.00\n",
              "merch_lat               0.00\n",
              "merch_long              0.00\n",
              "is_fraud                0.00\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#Checking the null values in train data\n",
        "\n",
        "100*df_fraud_train.isnull().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDHrITjbNBrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5b653d-6e11-43a6-e4f8-03d46198e3b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trans_date_trans_time   0.00\n",
              "cc_num                  0.00\n",
              "merchant                0.00\n",
              "category                0.00\n",
              "amt                     0.00\n",
              "first                   0.00\n",
              "last                    0.00\n",
              "gender                  0.00\n",
              "street                  0.00\n",
              "city                    0.00\n",
              "state                   0.00\n",
              "zip                     0.00\n",
              "lat                     0.00\n",
              "long                    0.00\n",
              "city_pop                0.03\n",
              "job                     0.03\n",
              "dob                     0.03\n",
              "trans_num               0.03\n",
              "unix_time               0.03\n",
              "merch_lat               0.03\n",
              "merch_long              0.03\n",
              "is_fraud                0.03\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Checking the null values in test data\n",
        "\n",
        "100*df_fraud_test.isnull().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUwFHAzMkbry"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrd9UsYIgGxX"
      },
      "outputs": [],
      "source": [
        "#As we can see that there are two columns for first name and last name\n",
        "# We will merege the first name and last name in train and test data\n",
        "df_fraud_train['Customer_Full_Name']=df_fraud_train[['first','last']].apply(lambda x:' '.join(x),axis=1)\n",
        "\n",
        "df_fraud_test['Customer_Full_Name']=df_fraud_test[['first','last']].apply(lambda x:' '.join(x),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjAERErsgG0P"
      },
      "outputs": [],
      "source": [
        "#Dropping the redundatnt first and last name col from train and test data\n",
        "df_fraud_train.drop(['first','last'],axis=1, inplace=True)\n",
        "\n",
        "df_fraud_test.drop(['first','last'],axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiaM5a5NrWOz"
      },
      "outputs": [],
      "source": [
        "# Now we will create age col by subtracting the DOB col from trans_date col DOB in train and test data\n",
        "df_fraud_train['Customer_Age']=df_fraud_train['trans_date_trans_time'].dt.year - df_fraud_train['dob'].dt.year\n",
        "\n",
        "df_fraud_test['Customer_Age']=df_fraud_test['trans_date_trans_time'].dt.year - df_fraud_test['dob'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izj9382BrWRo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "7656e25e-fb88-45ea-e804-3dd809f1d281"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8aae2b73e803>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Droppping the dob col from train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_fraud_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dob'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_fraud_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dob'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_fraud_train' is not defined"
          ]
        }
      ],
      "source": [
        "#Droppping the dob col from train and test data\n",
        "\n",
        "df_fraud_train.drop('dob',axis=1, inplace=True)\n",
        "\n",
        "df_fraud_test.drop('dob',axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW3kDJNErWXX"
      },
      "outputs": [],
      "source": [
        "# Now we will create few columns from trans_date_trans_time in train and test data for our further analysis\n",
        "df_fraud_train['Day_Of_Week']=df_fraud_train['trans_date_trans_time'].dt.dayofweek\n",
        "df_fraud_train['Day_Name']=df_fraud_train['trans_date_trans_time'].dt.day_name()\n",
        "df_fraud_train['No_Of_Week']=df_fraud_train['trans_date_trans_time'].dt.week\n",
        "df_fraud_train['No_Of_Day']=df_fraud_train['trans_date_trans_time'].dt.day\n",
        "df_fraud_train['Day_Hrs']=df_fraud_train['trans_date_trans_time'].dt.hour\n",
        "df_fraud_train['Month_Name']=df_fraud_train['trans_date_trans_time'].dt.month_name()\n",
        "df_fraud_train['Year']=df_fraud_train['trans_date_trans_time'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3900IglrWZ9"
      },
      "outputs": [],
      "source": [
        "# Now the same we will apply for test data\n",
        "df_fraud_test['Day_Of_Week']=df_fraud_test['trans_date_trans_time'].dt.dayofweek\n",
        "df_fraud_test['Day_Name']=df_fraud_test['trans_date_trans_time'].dt.day_name()\n",
        "df_fraud_test['No_Of_Week']=df_fraud_test['trans_date_trans_time'].dt.week\n",
        "df_fraud_test['No_Of_Day']=df_fraud_test['trans_date_trans_time'].dt.day\n",
        "df_fraud_test['Day_Hrs']=df_fraud_test['trans_date_trans_time'].dt.hour\n",
        "df_fraud_test['Month_Name']=df_fraud_test['trans_date_trans_time'].dt.month_name()\n",
        "df_fraud_test['Year']=df_fraud_test['trans_date_trans_time'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyYiTj4MrWiB"
      },
      "outputs": [],
      "source": [
        "# Now we will use haversine distance function to calculate the distance of base location of customer and location of transaction\n",
        "# Creating function to calculate the haversine distance\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "\n",
        "def haversine_dist(lon1, lat1, lon2, lat2):\n",
        "    # convert decimal degrees to radians\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    # haversine formula\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    r = 6371 * c # Radius of earth in kilometers.\n",
        "    return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpfkxraOrWng"
      },
      "outputs": [],
      "source": [
        "# Now we will calculate the distance of base location of customer and location of transaction for train data\n",
        "df_fraud_train['Dist_Cust_to_merchant']=haversine_dist(df_fraud_train['long'],df_fraud_train['lat'],df_fraud_train['merch_long'],df_fraud_train['merch_lat'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1eqUNzQrWp2"
      },
      "outputs": [],
      "source": [
        "# Now we will calculate the distance of base location of customer and location of transaction for test data\n",
        "df_fraud_test['Dist_Cust_to_merchant']=haversine_dist(df_fraud_test['long'],df_fraud_test['lat'],df_fraud_test['merch_long'],df_fraud_test['merch_lat'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5PycaoHlmw6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "16afc5eb-e1d4-49e7-a2e8-72aa10810564"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8321cc41fceb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Now we will create the Age bucket for train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_fraud_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age_Bucket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fraud_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Customer_Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m39\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m59\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<30'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'30-40'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'40-50'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'50-60'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'60>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Creating the Age bucket for test data as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_fraud_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age_Bucket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fraud_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Customer_Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m39\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m59\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<30'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'30-40'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'40-50'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'50-60'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'60>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_fraud_train' is not defined"
          ]
        }
      ],
      "source": [
        "#Now we will create the Age bucket for train and test data\n",
        "df_fraud_train['Age_Bucket']=pd.cut(df_fraud_train['Customer_Age'],[0,29,39,49,59,9999],labels=['<30','30-40','40-50','50-60','60>'])\n",
        "\n",
        "#Creating the Age bucket for test data as well\n",
        "df_fraud_test['Age_Bucket']=pd.cut(df_fraud_test['Customer_Age'],[0,29,39,49,59,9999],labels=['<30','30-40','40-50','50-60','60>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0wlu_Aomirh"
      },
      "outputs": [],
      "source": [
        "df_fraud_train.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ6nlD9ysfnV"
      },
      "outputs": [],
      "source": [
        "#Checking the shape for train and test\n",
        "df_fraud_train.shape, df_fraud_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN0Pe2QDyHh5"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Cf-bb_X2gA"
      },
      "outputs": [],
      "source": [
        "# Now we will analyze the target variable. We will check the count of class 1 and 0\n",
        "#CHecking for train data\n",
        "sns.countplot(df_fraud_train['is_fraud'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YFioYLRrK7P"
      },
      "outputs": [],
      "source": [
        "#Checking for test data\n",
        "sns.countplot(df_fraud_test['is_fraud'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "H5UZvNrGrpX_",
        "outputId": "3d73200b-b81b-4d3e-8b5b-e8bf01eaeade"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2c5eed9df6e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Checking the percentage of fraudulent vs non fraudulent transaction in train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fraud_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_fraud'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Checking for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fraud_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_fraud'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_fraud_train' is not defined"
          ]
        }
      ],
      "source": [
        "#Checking the percentage of fraudulent vs non fraudulent transaction in train and test data\n",
        "print((df_fraud_train['is_fraud'].value_counts(normalize=True)*100))\n",
        "\n",
        "#Checking for test data\n",
        "print((df_fraud_test['is_fraud'].value_counts(normalize=True)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBv69H71q4j4"
      },
      "source": [
        "- As we can see that from above in train and test data fraudulent transaction has very low entries which is very obvious but it is a case of class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anhmGTU6X2l8"
      },
      "outputs": [],
      "source": [
        "# Now we will see the counts of gender in train data\n",
        "df_fraud_train['gender'].value_counts(normalize=True).plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZACM9qq-z_P"
      },
      "source": [
        "- As we can see that female count is more than male count in training data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXs_4L70X2rK"
      },
      "outputs": [],
      "source": [
        "#Now we will see the distribution of amount column in training data\n",
        "sns.distplot(df_fraud_train['amt'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFFByyj0BPF8"
      },
      "source": [
        "- As we can see data is skewed so we will plot the boxplot to see if there is any outlier present in Amt column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI3YTx-yDH1h"
      },
      "outputs": [],
      "source": [
        "#Checking outliers for the numerical col\n",
        "df_train_num=df_fraud_train[['amt','city_pop','Customer_Age','Day_Of_Week','No_Of_Week','No_Of_Day','Day_Hrs','Dist_Cust_to_merchant']]\n",
        "df_train_num.describe(percentiles=[.25,.5,.75,.90,.95,.99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llm89vkrPx3S"
      },
      "source": [
        "- As we can see from describe that amt data is skewed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT_rHjPqCOgL"
      },
      "outputs": [],
      "source": [
        "# Plotting the subplots to check the outliers analysis\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.subplot(2,4,1)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='amt',hue='is_fraud')\n",
        "plt.legend(loc='lower center')\n",
        "\n",
        "plt.subplot(2,4,2)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='city_pop',hue='is_fraud')\n",
        "\n",
        "plt.subplot(2,4,3)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='Customer_Age',hue='is_fraud')\n",
        "\n",
        "plt.subplot(2,4,4)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='Day_Of_Week',hue='is_fraud')\n",
        "\n",
        "plt.subplot(2,4,5)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='No_Of_Week',hue='is_fraud')\n",
        "\n",
        "plt.subplot(2,4,6)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='No_Of_Day',hue='is_fraud')\n",
        "\n",
        "plt.subplot(2,4,7)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='Day_Hrs',hue='is_fraud')\n",
        "plt.legend(loc='lower center')\n",
        "\n",
        "plt.subplot(2,4,8)\n",
        "sns.boxplot(data=df_fraud_train,x='is_fraud',y='Dist_Cust_to_merchant',hue='is_fraud')\n",
        "\n",
        "plt.legend(loc='lower center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxuTzdQUeW8X"
      },
      "source": [
        "- As we can see that customer who are non fraudulent having extreme values in amount col which can happen as most of the customer can have high salary which in turn can purchase the product of high amount\n",
        "- Also city population having the outlier which can happen as population may vary city to city and we did not find any extreme surprise numbers.\n",
        "- We can see that max fraudulent transaction has occured in the mid night.\n",
        "- Rest of the data seems to be fine but few data seems to be skewed.\n",
        "- So we will not remove the oultiers as they are important for our further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZK69MPItUqR"
      },
      "source": [
        "# Bi-Variate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIstgmAXX23p"
      },
      "outputs": [],
      "source": [
        "#Plotting the barplot for amt col with respect to target variable\n",
        "sns.barplot(data=df_fraud_train,x='is_fraud',y='amt')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OskrQMu4thHl"
      },
      "source": [
        "- As we can see that from above barplot that number of fraudulent transaction are more as compare to non fradulent transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbQHXQLyX25_"
      },
      "outputs": [],
      "source": [
        "# Now we will analyse the gender col with respect to target variable\n",
        "sns.countplot(data=df_fraud_train, x='gender',hue='is_fraud')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnptHH6hwxn3"
      },
      "outputs": [],
      "source": [
        "# Now we will analyse the gender col with respect to target variable\n",
        "df_fraud_train.groupby(by=['gender'])['is_fraud'].sum().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg_9EK7V0uBD"
      },
      "outputs": [],
      "source": [
        "df_fraud_train.groupby(by=['gender'])['is_fraud'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bvrcm-gyV_e"
      },
      "source": [
        "- As we can see that there are not so much difference between gender with respect to fraudulent transaction. However numbers are less but it is done by both the genders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY3eLWMZyVUv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "df_statewise=df_fraud_train.groupby(by=['state'])['is_fraud'].sum()\n",
        "df_fraud_statewise=df_statewise.sort_values(ascending=False).head(10)\n",
        "df_fraud_statewise.plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAE9AGKe1lo9"
      },
      "source": [
        "- From above we can see that NY state has having the highest number of fraudulent transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnVpN2LXyVXa"
      },
      "outputs": [],
      "source": [
        "#Now we will analyse the fraudulent transactions with respect to category\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "df_category=df_fraud_train.groupby(by=['category'])['is_fraud'].sum()\n",
        "df_fraud_category=df_category.sort_values(ascending=False)\n",
        "df_fraud_category.plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biL9Uxyu3zeq"
      },
      "source": [
        "- As we can see that highest number of fraudulent transaction occured in categories Grocery_pos followed by shopping_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lgqRAyX2yqv"
      },
      "outputs": [],
      "source": [
        "#Now we will analyse the fraudulent transaction with respect to weekday\n",
        "plt.figure(figsize=(10,5))\n",
        "df_fraud_train.groupby(by=['Day_Name'])['is_fraud'].sum().sort_values(ascending=False).plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPlH9VfE4z_7"
      },
      "source": [
        "- From above chart maximum fraudulent transaction occured in Saturday and Sunday followed by Monday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81tlX7KO2yth"
      },
      "outputs": [],
      "source": [
        "#In this graph will analyse the fraudulent transaction with respect to Month\n",
        "plt.figure(figsize=(10,5))\n",
        "df_fraud_train.groupby(by=['Month_Name'])['is_fraud'].sum().sort_values(ascending=False).plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC0kI7QM5ZIO"
      },
      "source": [
        "- Most of the fraudulent transaction happend in the month of March and May"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gswtImxG2yyu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.distplot(df_fraud_train[df_fraud_train['is_fraud'] == 0][\"Day_Hrs\"])\n",
        "plt.title('Non-Fraudulent Transaction')\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df_fraud_train[df_fraud_train['is_fraud'] == 1][\"Day_Hrs\"])\n",
        "plt.title('Fraudulent Transaction')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8b3kdN-_1i1"
      },
      "source": [
        "- From above we can see that fraudulent transaction happend in odd hrs which is obvius expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hQOcPNK_-VO"
      },
      "outputs": [],
      "source": [
        "#Now we will check the agebucket with respect to target variable\n",
        "df_fraud_train[df_fraud_train['is_fraud']==1]['Age_Bucket'].value_counts(normalize=True).plot.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Deu3sQO0RORW"
      },
      "source": [
        "# Data Preparation For Data Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8sByscZLPSO"
      },
      "outputs": [],
      "source": [
        "#Checking the head\n",
        "df_fraud_train.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df_Ied73LPVk"
      },
      "outputs": [],
      "source": [
        "#Creating copy of training data and test data\n",
        "df_fraud_train_model=df_fraud_train.copy()\n",
        "df_fraud_test_model=df_fraud_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGDtB1FNLPYb"
      },
      "outputs": [],
      "source": [
        "#Checking the shape\n",
        "df_fraud_train_model.shape, df_fraud_test_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG6OuUmJLPbj"
      },
      "outputs": [],
      "source": [
        "#Plotting the heatmap to check the correlation\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.heatmap(df_fraud_train_model.corr(),annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZoW2IsoLPea"
      },
      "outputs": [],
      "source": [
        "# So now we will drop the highly correlated features from the dataset\n",
        "#now we are creating a square matrix with dimensions equal to the number of features. In which we will have the elements as the absolute value of correlation between the features.\n",
        "cor_matrix = df_fraud_train_model.corr().abs()\n",
        "print(cor_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owRfwDZxLPhB"
      },
      "outputs": [],
      "source": [
        "#Correlation matrix will be mirror image about the diagonal and all the diagonal elements will be 1.\n",
        "#we are selecting the upper traingular\n",
        "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
        "print(upper_tri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xAmZCXYLPjt"
      },
      "outputs": [],
      "source": [
        "#So we are selecting the columns which are having absolute correlation greater than 0.70 and making a list of those columns named 'to_drop'.\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.70)]\n",
        "print(to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2naXb4_l7tPT"
      },
      "outputs": [],
      "source": [
        "# Now dropping the highly correlated col for test dataset\n",
        "cor_matrix_1 = df_fraud_test_model.corr().abs()\n",
        "upper_tri_1 = cor_matrix_1.where(np.triu(np.ones(cor_matrix_1.shape),k=1).astype(np.bool))\n",
        "to_drop_1 = [column for column in upper_tri_1.columns if any(upper_tri_1[column] > 0.70)]\n",
        "print(to_drop_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hpMGQcC7tSH"
      },
      "outputs": [],
      "source": [
        "#Dropping the selected highly correlated columns list\n",
        "df_fraud_train_model.drop(to_drop,axis=1,inplace=True)\n",
        "df_fraud_test_model.drop(to_drop_1,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4TpRd1G7tU4"
      },
      "outputs": [],
      "source": [
        "#Checking the shape of train and test data\n",
        "df_fraud_train_model.shape, df_fraud_test_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TobEibf97tXU"
      },
      "outputs": [],
      "source": [
        "#Checking the columns which has to drop\n",
        "df_fraud_train_model.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d26wP2NPLPmg"
      },
      "outputs": [],
      "source": [
        "#Dropping the irrelevant train dataset columns\n",
        "df_fraud_train_model.drop(['trans_date_trans_time', 'cc_num', 'merchant' ,'street', 'city', 'state', 'zip', 'lat', 'job',\n",
        "                           'trans_num','Unix_Time_Stamp', 'Customer_Full_Name','Day_Of_Week', 'Day_Name', 'No_Of_Week', 'No_Of_Day','Day_Hrs','Month_Name','Year','Age_Bucket'],axis=1,inplace=True)\n",
        "#Dropping the irrelevant test dataset columns\n",
        "df_fraud_test_model.drop(['trans_date_trans_time', 'cc_num', 'merchant', 'street', 'city', 'state', 'zip', 'lat', 'job',\n",
        "                           'trans_num','Unix_Time_Stamp', 'Customer_Full_Name','Day_Of_Week', 'Day_Name', 'No_Of_Week', 'No_Of_Day','Day_Hrs','Month_Name','Year','Age_Bucket'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXCP-1FEBPuY"
      },
      "outputs": [],
      "source": [
        "df_fraud_train_model.shape, df_fraud_test_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l51JzHKBBPxw"
      },
      "outputs": [],
      "source": [
        "# Now we will change the categorical columns to binary 0 and 1\n",
        "df_fraud_train_model.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPL4FNmjBP0U"
      },
      "outputs": [],
      "source": [
        "#We will create a dummy variable for some of the categorical variables and dropping the first one\n",
        "dummy_1= pd.get_dummies(df_fraud_train_model[['category','gender']],drop_first=True)\n",
        "dummy_2=pd.get_dummies(df_fraud_test_model[['category','gender']],drop_first=True)\n",
        "\n",
        "#Adding the results to the master dataframe\n",
        "df_fraud_train_model=pd.concat([df_fraud_train_model,dummy_1],axis=1)\n",
        "df_fraud_test_model=pd.concat([df_fraud_test_model,dummy_2],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXWzL7PyBP27"
      },
      "outputs": [],
      "source": [
        "#Dropping the repeated col\n",
        "df_fraud_train_model.drop(['category','gender'],axis=1,inplace=True)\n",
        "df_fraud_test_model.drop(['category','gender'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJQrzlR8BP8R"
      },
      "outputs": [],
      "source": [
        "df_fraud_train_model.shape,df_fraud_test_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQH7GwohPhlO"
      },
      "outputs": [],
      "source": [
        "#Splitting the data into X and y\n",
        "X_training=df_fraud_train_model.drop('is_fraud',axis=1)\n",
        "y_training=df_fraud_train_model['is_fraud']\n",
        "\n",
        "x_testing=df_fraud_test_model.drop('is_fraud',axis=1)\n",
        "y_testing=df_fraud_test_model['is_fraud']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryHt4N29us0q"
      },
      "outputs": [],
      "source": [
        "# Lets check the distribution before scaling\n",
        "plt.figure(figsize=[15,8])\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('Distribution of Amount', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_training['amt'])\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Distribution of Customer_Age', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_training['Customer_Age'])\n",
        "plt.subplot(2,2,3)\n",
        "plt.title('Distribution_of_distance_Customer_to_merchant)', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_training['Dist_Cust_to_merchant'])\n",
        "plt.subplot(2,2,4)\n",
        "plt.title('Distribution of city_pop', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_training['city_pop'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz_h76R6G1Ng"
      },
      "source": [
        "- As we can see that data is skewed and we have to convert it normal distribution before passing it to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC0RJttzus_B"
      },
      "outputs": [],
      "source": [
        "#Splitting the training data into train validation split\n",
        "X_train, X_valid, y_train, y_valid=train_test_split(X_training,y_training, train_size=.70,stratify=y_training, random_state=42)\n",
        "\n",
        "#Checking the shape\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbEiMA2fZpdx"
      },
      "outputs": [],
      "source": [
        "#Scaling the training data through Power transformation\n",
        "scaler_PT=PowerTransformer()\n",
        "#We will fit and transform the training data\n",
        "X_train[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']]=scaler_PT.fit_transform(X_train[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']])\n",
        "\n",
        "#And we will only transform the validation set\n",
        "\n",
        "X_valid[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']]=scaler_PT.transform(X_valid[['amt','city_pop','Customer_Age','Dist_Cust_to_merchant']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdiQXBoztXmh"
      },
      "outputs": [],
      "source": [
        "# Lets check the distribution after scaling\n",
        "plt.figure(figsize=[15,8])\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('Distribution of Amount', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_train['amt'])\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Distribution of Customer_Age', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_train['Customer_Age'])\n",
        "plt.subplot(2,2,3)\n",
        "plt.title('Distribution_of_Customer_to_merchant)', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_train['Dist_Cust_to_merchant'])\n",
        "plt.subplot(2,2,4)\n",
        "plt.title('Distribution of city_pop', fontsize= 10, color = 'Red', fontweight = 100)\n",
        "plt.hist(X_train['city_pop'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4kA5Jz6DkwU"
      },
      "source": [
        "- As we can see that data has been quite normally distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hFtjZ50Phok"
      },
      "outputs": [],
      "source": [
        "# We will apply two class imbalance techniques for our model\n",
        "#Now we will apply SMOTE to balance the class\n",
        "from collections import Counter\n",
        "counter= Counter(y_train)\n",
        "print('Before Applying SMOTE', counter)\n",
        "smt=SMOTE(sampling_strategy={1:555555}, random_state=42)\n",
        "x_training_smte,y_training_smte=smt.fit_resample(X_train,y_train)\n",
        "counter = Counter(y_training_smte)\n",
        "print('After Applying SMOTE', counter)\n",
        "\n",
        "\n",
        "#We will apply ADASYN to balance the class\n",
        "counter_1= Counter(y_train)\n",
        "print('Before Applying ADASYN', counter_1)\n",
        "ada = ADASYN(sampling_strategy={1:555555},random_state=42)\n",
        "X_training_ada, y_training_ada = ada.fit_resample(X_train, y_train)\n",
        "counter_1 = Counter(y_training_ada)\n",
        "print('After Applying ADASYN', counter_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG7dVCjIPhr-"
      },
      "outputs": [],
      "source": [
        "#checking the shape after class balance\n",
        "x_training_smte.shape,y_training_smte.shape, X_training_ada.shape, y_training_ada.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DpiJ0iUPhun"
      },
      "outputs": [],
      "source": [
        "X_train.shape,y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kguz_Pz196wN"
      },
      "outputs": [],
      "source": [
        "#Checking the VIF for attributes if it is not so importance we will drop that columns\n",
        "col=x_training_smte.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj1AE4t12tXr"
      },
      "outputs": [],
      "source": [
        "# We will use the VIF to check the import variables and remove the redundant variable basis on the VIF score\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "#Creating the data frame theat will contain all features and their respective VIF's\n",
        "vif=pd.DataFrame()\n",
        "vif['features'] = x_training_smte[col].columns\n",
        "vif['VIF'] = [variance_inflation_factor(x_training_smte[col].values,i) for i in range (x_training_smte[col].shape[1])]\n",
        "vif['VIF'] = round(vif['VIF'],2)\n",
        "vif = vif.sort_values(by= 'VIF', ascending = False)\n",
        "vif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKTEfAb2UL-n"
      },
      "source": [
        "- As we can see that all the VIF values are less than 5 so we will not drop any features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oE0Kh_8FUjg"
      },
      "source": [
        "### Creating Model Algorithm & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSdIMiIBTwhU"
      },
      "outputs": [],
      "source": [
        "#Creating an instance of Algorithm for Classification models\n",
        "#Logistic Regression\n",
        "model_LR=LogisticRegression()\n",
        "\n",
        "#Decision Tree\n",
        "model_DT = DecisionTreeClassifier(random_state = 42)\n",
        "\n",
        "#Random Forest\n",
        "model_RF = RandomForestClassifier(oob_score= True,random_state=42)\n",
        "\n",
        "#XGBoost\n",
        "model_XGB = XGBClassifier()\n",
        "\n",
        "#XGBoost Ray\n",
        "#model_XGBR= RayXGBClassifier( n_jobs=4,random_state = 'seed')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTy4bT-26Rd5"
      },
      "outputs": [],
      "source": [
        "#creating an instance of Stratified K Folf CV and taking n_splits as 5\n",
        "skfold=StratifiedKFold(n_splits=5)\n",
        "\n",
        "#Creating hyper parameter tuning for logistic regression\n",
        "params_LR = {\n",
        "'solver': ['liblinear'],\n",
        "'penalty' : ['l1','l2'],\n",
        "'C' : np.logspace(-1, 5, 10),\n",
        "'class_weight' : [None, 'balanced']}\n",
        "\n",
        "\n",
        "#Creating hyper parameter tuning for Decision Tree\n",
        "params_DT = {\n",
        "'max_depth': [3,5,10,None],\n",
        "'min_samples_leaf': [100,150,200],\n",
        "'max_features' : randint(1,5),\n",
        "'min_samples_split' : [100,200,300],\n",
        "'criterion': [\"gini\",\"entropy\"]}\n",
        "\n",
        "#Creating hyper parameter tuning for Random Forest\n",
        "params_RF = {\n",
        "'max_depth': [3,5,10,None],\n",
        "'n_estimators' : [10,15,20],\n",
        "'max_features' : randint(1,6),\n",
        "'min_samples_leaf': [100,150,200],\n",
        "'min_samples_split' : [100,200,300],\n",
        "'bootstrap' : [True, False],\n",
        "'criterion': ['gini','entropy']}\n",
        "\n",
        "#Creating hyper parameter tuning for XGBoost\n",
        "params_XGB = {\n",
        "'learning_rate' : [0.05, 0.1],\n",
        "'max_depth': [3,5],\n",
        "#'min_child_weight' : [1,3,5],\n",
        "'gamma' : [0.1,0.2],\n",
        "'max_features': randint(1,5),\n",
        "'method':     ['approx','hist']}\n",
        "#'colsample_bytree': np.arange(0.4, 1.0, 0.1)}\n",
        "\n",
        "#Creating hyper parameter tuning for XGBoost Ray\n",
        "#params_XGBR= {\n",
        "#'learning_rate' : [0.05, 0.1],\n",
        "#'max_depth': [3,5],\n",
        "#'gamma' : [0.1,0.2],\n",
        "#'max_features': randint(1,5),\n",
        "#'method':     ['hist']}\n",
        "\n",
        "#Since XGBoost & XGBoost Ray is taking computational longer time to run so we have to avoid using it for this problem statement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adIZKWibYIRn"
      },
      "outputs": [],
      "source": [
        "# Creating function to plot ROC curve\n",
        "def plot_roc(actual, proba):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(actual, proba, drop_intermediate = False )\n",
        "    auc_score = metrics.roc_auc_score(actual, proba)\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.plot( fpr, tpr, label='ROC Curve (area = %0.2f)' % auc_score )\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZdFzvXCZLUz"
      },
      "outputs": [],
      "source": [
        "#Creating function to for variuos metrices for a classification model\n",
        "def model_metrics(a, b):\n",
        "  confusion = confusion_matrix(a, b)\n",
        "  TP = confusion[1,1] # true positive\n",
        "  TN = confusion[0,0] # true negatives\n",
        "  FP = confusion[0,1] # false positives\n",
        "  FN = confusion[1,0] # false negatives\n",
        "  print ('Accuracy    : ', metrics.accuracy_score(a, b))\n",
        "  print ('Sensitivity : ', TP / float(TP+FN))\n",
        "  print ('Specificity : ', TN / float(TN+FP))\n",
        "  print ('Precision   : ', TP / float(TP + FP))\n",
        "  print ('Recall      : ', TP / float(TP + FN))\n",
        "  print('F1_Score:',metrics.f1_score(a,b))\n",
        "  print(confusion)\n",
        "\n",
        "\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cApN61Ygkjd"
      },
      "source": [
        "# Logistic Regression Without Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmD5_5HKZsIh"
      },
      "outputs": [],
      "source": [
        "#Creating 1st model of Logistic Regression without Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "LR_RCV = RandomizedSearchCV(model_LR, params_LR, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "LR_RCV.fit(X_train, y_train)\n",
        "print(LR_RCV.best_estimator_)\n",
        "print(LR_RCV.best_params_)\n",
        "print(LR_RCV.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMXfWhJFcebc"
      },
      "outputs": [],
      "source": [
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = LR_RCV.best_estimator_.predict(X_train)\n",
        "y_test_val_pred = LR_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_train, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_train, y_train_pred))\n",
        "model_metrics(y_train, y_train_pred)\n",
        "print('*'*70)\n",
        "\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print(\"Validation_Data_metrics\")\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPQaNV5Jg1iw"
      },
      "source": [
        "# Decision Tree Without Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l0lBO1Bg5FR"
      },
      "outputs": [],
      "source": [
        "#Creating 2nd model of Decision Tree without Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "DT_RCV = RandomizedSearchCV(model_DT, params_DT, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "DT_RCV.fit(X_train, y_train)\n",
        "print(DT_RCV.best_estimator_)\n",
        "print(DT_RCV.best_params_)\n",
        "print(DT_RCV.best_score_)\n",
        "\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = DT_RCV.best_estimator_.predict(X_train)\n",
        "y_test_val_pred = DT_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_train, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_train, y_train_pred))\n",
        "model_metrics(y_train, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz0IYwXzht9e"
      },
      "source": [
        "# Random Forest Without Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpRqLeJ7g5Uq"
      },
      "outputs": [],
      "source": [
        "#Creating 3rd model of Random Forest without Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "RF_RCV = RandomizedSearchCV(model_RF, params_RF, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "RF_RCV.fit(X_train, y_train)\n",
        "print(RF_RCV.best_estimator_)\n",
        "print(RF_RCV.best_params_)\n",
        "print(RF_RCV.best_score_)\n",
        "\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = RF_RCV.best_estimator_.predict(X_train)\n",
        "y_test_val_pred = RF_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_train, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_train, y_train_pred))\n",
        "model_metrics(y_train, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BkiEYb_ig1e"
      },
      "source": [
        "# XGBoost without Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsPlx16CiU59"
      },
      "outputs": [],
      "source": [
        "#Creating 4th model of XGBoost without Sampling\n",
        "\n",
        "#creating an instance of Stratified K Folf CV and taking n_splits as 3 as XGBoost is taking time run the model\n",
        "#skfold=StratifiedKFold(n_splits=3)\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "#XGB_RCV = RandomizedSearchCV(model_XGB, params_XGB, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "#XGB_RCV.fit(X_train, y_train)\n",
        "#print(XGB_RCV.best_estimator_)\n",
        "#print(XGB_RCV.best_params_)\n",
        "#print(XGB_RCV.best_score_)\n",
        "\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "#y_train_pred = XGB_RCV.best_estimator_.predict(X_train)\n",
        "#y_test_val_pred = XGB_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "#plot_roc(y_train, y_train_pred)\n",
        "#print(\"Training_Data_Metrics\")\n",
        "#print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_train, y_train_pred))\n",
        "#model_metrics(y_train, y_train_pred)\n",
        "#print('*'*70)\n",
        "#print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "#plot_roc(y_valid, y_test_val_pred)\n",
        "#print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "#model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEiFVdtFJsKe"
      },
      "source": [
        "#### Applied XGBoost without Sampling technique for model building but the model is taking computational lot of time to run, so we are excluding this model in our project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaiolPr9O5X9"
      },
      "source": [
        "# Logistic Regression With SMOTE Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNIjb0FBiU_n"
      },
      "outputs": [],
      "source": [
        "#Creating 5th model of Logistic Regression with SMOTE Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "LR_RCV = RandomizedSearchCV(model_LR, params_LR, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "LR_RCV.fit(x_training_smte, y_training_smte)\n",
        "print(LR_RCV.best_estimator_)\n",
        "print(LR_RCV.best_params_)\n",
        "print(LR_RCV.best_score_)\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = LR_RCV.best_estimator_.predict(x_training_smte)\n",
        "y_test_val_pred = LR_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_training_smte, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_training_smte, y_train_pred))\n",
        "model_metrics(y_training_smte, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsXst4xHQWYW"
      },
      "source": [
        "# Decision Tree With SMOTE Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zisvXkP2PcXp"
      },
      "outputs": [],
      "source": [
        "#Creating 6th model of Decision Tree with SMOTE Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "DT_RCV = RandomizedSearchCV(model_DT, params_DT, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "DT_RCV.fit(x_training_smte, y_training_smte)\n",
        "print(DT_RCV.best_estimator_)\n",
        "print(DT_RCV.best_params_)\n",
        "print(DT_RCV.best_score_)\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = DT_RCV.best_estimator_.predict(x_training_smte)\n",
        "y_test_val_pred = DT_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_training_smte, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_training_smte, y_train_pred))\n",
        "model_metrics(y_training_smte, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NKcqIifQqiW"
      },
      "source": [
        "# Random Forest With SMOTE Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvkzLeJeiVCj"
      },
      "outputs": [],
      "source": [
        "#Creating 7th model of Random Forest with SMOTE Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "RF_RCV = RandomizedSearchCV(model_RF, params_RF, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "RF_RCV.fit(x_training_smte, y_training_smte)\n",
        "print(RF_RCV.best_estimator_)\n",
        "print(RF_RCV.best_params_)\n",
        "print(RF_RCV.best_score_)\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = RF_RCV.best_estimator_.predict(x_training_smte)\n",
        "y_test_val_pred = RF_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_training_smte, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_training_smte, y_train_pred))\n",
        "model_metrics(y_training_smte, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG_4Fv-lmtNb"
      },
      "source": [
        "- Since XGBoost classifier is taking computational longer time to run so we are avoiding it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVs6symkTD1c"
      },
      "source": [
        "# Logistic Regression With ADASYN Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye0nRZXWiVIV"
      },
      "outputs": [],
      "source": [
        "#Creating 8th model of Logistic Regression with ADASYN Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "LR_RCV = RandomizedSearchCV(model_LR, params_LR, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "LR_RCV.fit(X_training_ada, y_training_ada)\n",
        "print(LR_RCV.best_estimator_)\n",
        "print(LR_RCV.best_params_)\n",
        "print(LR_RCV.best_score_)\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = LR_RCV.best_estimator_.predict(X_training_ada)\n",
        "y_test_val_pred = LR_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_training_ada, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_training_ada, y_train_pred))\n",
        "model_metrics(y_training_ada, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh81MhMJUALq"
      },
      "source": [
        "# Decision Tree With ADASYN Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_u-AuuWT-7_"
      },
      "outputs": [],
      "source": [
        "#Creating 9th model of Decision Tree with ADASYN Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "DT_RCV = RandomizedSearchCV(model_DT, params_DT, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "DT_RCV.fit(X_training_ada, y_training_ada)\n",
        "print(DT_RCV.best_estimator_)\n",
        "print(DT_RCV.best_params_)\n",
        "print(DT_RCV.best_score_)\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = DT_RCV.best_estimator_.predict(X_training_ada)\n",
        "y_test_val_pred = DT_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_training_ada, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_training_ada, y_train_pred))\n",
        "model_metrics(y_training_ada, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyGCly_0UX97"
      },
      "source": [
        "# Random Forest With ADASYN Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Bz47pXUcW6"
      },
      "outputs": [],
      "source": [
        "#Creating 10th model of Random Forest with ADASYN Sampling\n",
        "#Using Randomzided Search CV techniques to get the best parameter for the model building\n",
        "RF_RCV = RandomizedSearchCV(model_RF, params_RF, cv=skfold, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "\n",
        "#Fitting the model\n",
        "RF_RCV.fit(X_training_ada, y_training_ada)\n",
        "print(RF_RCV.best_estimator_)\n",
        "print(RF_RCV.best_params_)\n",
        "print(RF_RCV.best_score_)\n",
        "\n",
        "#Getting the predicted values on the train set and validation set\n",
        "y_train_pred = RF_RCV.best_estimator_.predict(X_training_ada)\n",
        "y_test_val_pred = RF_RCV.best_estimator_.predict(X_valid)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_training_ada, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_training_ada, y_train_pred))\n",
        "model_metrics(y_training_ada, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Validation_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Validation Data\n",
        "plot_roc(y_valid, y_test_val_pred)\n",
        "print ('AUC for the Validation_Data: ', metrics.roc_auc_score( y_valid, y_test_val_pred))\n",
        "model_metrics(y_valid, y_test_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eMyrV2lrKmB"
      },
      "source": [
        "# Evaluation Of The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp0JE3I0rQjQ"
      },
      "source": [
        "######As we have seen above that Decicion Tree and Random Forest algorith has performed very well on training data and validation data after balancing the class thorugh SMOTE and ADASYN technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimSE4jZal5V"
      },
      "source": [
        "- #### So we will now apply the Random Forest With ADASYN technique trained model on test data set to check if the model is performing ver well or not.\n",
        "- #### As we have seen above that the Random Forest With ADASYN has given the fantastic results more than 95% of score. Also the Recall on validation data is approx 95% which is good to go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns6tu0OJTp4A"
      },
      "outputs": [],
      "source": [
        "#Splitting the data into X and y\n",
        "X_train_final=df_fraud_train_model.drop('is_fraud',axis=1)\n",
        "y_train_final=df_fraud_train_model['is_fraud']\n",
        "\n",
        "X_test_final=df_fraud_test_model.drop('is_fraud',axis=1)\n",
        "y_test_final=df_fraud_test_model['is_fraud']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLsnzfXSTp6w"
      },
      "outputs": [],
      "source": [
        "#Before testing the model on test data set we will have to power tranform the test data\n",
        "#But we will only transform the test rather than fit and transform\n",
        "#Scaling the training and testing data through Power transformation\n",
        "X_train_final[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']]=scaler_PT.fit_transform(X_train_final[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']])\n",
        "\n",
        "#We will transform the testing data with scaler_PT.transform\n",
        "X_test_final[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']]=scaler_PT.transform(X_test_final[['amt',\t'city_pop','Customer_Age','Dist_Cust_to_merchant']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CefWPAylokDJ"
      },
      "outputs": [],
      "source": [
        "#We will apply ADASYN to balance the class before passing it to train the model\n",
        "counter= Counter(y_train_final)\n",
        "print('Before Applying ADASYN', counter)\n",
        "ada = ADASYN(sampling_strategy={1:555555},random_state=42)\n",
        "X_train_ada, y_train_ada = ada.fit_resample(X_train_final, y_train_final)\n",
        "counter = Counter(y_train_ada)\n",
        "print('After Applying ADASYN', counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enVJfciPjDpE"
      },
      "source": [
        "###### Building the Final Model on the best parameter of Random Forest With ADASYN Sampling on whole training data set and then we will apply the model for prediction on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATR7X6NZtKPt"
      },
      "outputs": [],
      "source": [
        "#Creating the function to check the how much time Model is taking to training the data\n",
        "def timer(start_time=None):\n",
        "  if not start_time:\n",
        "    start_time =  datetime.now()\n",
        "    return start_time\n",
        "  elif start_time:\n",
        "    thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(),3600)\n",
        "    tmin, tsec = divmod(temp_sec, 60)\n",
        "    print('\\n Time taken : %i hours %i min and %i sec.' %(thour, tmin, round(tsec,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5358rD7tTp96"
      },
      "outputs": [],
      "source": [
        "#Applying Random Forest With ADASYN Sampling method on test data\n",
        "#Creating Final model of Random Forest with ADASYN Sampling on whole training data\n",
        "\n",
        "Final_model_RF = RandomForestClassifier(oob_score= True,criterion='entropy', max_features=4,min_samples_leaf=200, min_samples_split=300,\n",
        "                                  n_estimators=10,random_state=42)\n",
        "#Here starts the time\n",
        "start_time = timer(None)\n",
        "#Fitting the Final Model with best parameter on training data\n",
        "Final_model_RF.fit(X_train_ada, y_train_ada)\n",
        "\n",
        "#Here ends the time\n",
        "timer(start_time)\n",
        "\n",
        "#Getting the predicted values on the train set and test set\n",
        "y_train_pred = Final_model_RF.predict(X_train_ada)\n",
        "y_test_pred = Final_model_RF.predict(X_test_final)\n",
        "\n",
        "#Plotting the AUC ROC Curve for training data\n",
        "plot_roc(y_train_ada, y_train_pred)\n",
        "print(\"Training_Data_Metrics\")\n",
        "print ('AUC for the Training_Data: ', metrics.roc_auc_score( y_train_ada, y_train_pred))\n",
        "model_metrics(y_train_ada, y_train_pred)\n",
        "print('*'*70)\n",
        "print(\"Test_Data_metrics\")\n",
        "\n",
        "#Plotting the AUC ROC Curve for Test Data\n",
        "plot_roc(y_test_final, y_test_pred)\n",
        "print ('AUC for the Test_Data: ', metrics.roc_auc_score( y_test_final, y_test_pred))\n",
        "model_metrics(y_test_final, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWEVG22HEMcH"
      },
      "source": [
        "- So from above final model building on whole training data set and running on test data for prediction we can see the very good results. And the results are as:\n",
        "\n",
        " ##### 1) AUC on Test Data - 96%\n",
        "\n",
        " ##### 2) Accuracy on Test Data - 98%\n",
        "\n",
        " ##### 3) Senstivity on Test Data - 94% (Also known as Recall)\n",
        "\n",
        " ##### 4) Precision on Test Data - 15%\n",
        "\n",
        " ##### 5) Recall on Test Data - 94%\n",
        "\n",
        " - As we understood from the modules that 'Accuracy' is not always the correct metric for solving classification problems of imbalanced data. Metrics like confusion matrix, precision, recall, F1 score which are threshold dependent whereas AUC-ROC score evaluates the performance of the model at all the classification thresholds.\n",
        "\n",
        " - Therefore in this problem statment, for banks having a larger transaction value. So here, to save banks from high-value fraudulent transactions, we have to focus on a high recall in order to detect actual fraudulent transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpO3tMh-Y0Ko"
      },
      "outputs": [],
      "source": [
        "# Displaying the top features importance\n",
        "feature_imp = pd.DataFrame({\n",
        "    \"Col_Name\": X_train_ada.columns,\n",
        "    \"Feature_Imp\": Final_model_RF.feature_importances_})\n",
        "\n",
        "feature_imp.sort_values(by=\"Feature_Imp\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9dwbzzLcRNa"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "- To save banks from high-value fraudulent transactions, we have to focus on a high recall in order to detect actual fraudulent transactions.\n",
        "- In our model we have got the Recall as 94% which is good to go to deploy this model and the AUC is 96% which means model can classify fraudulent and non fraudulent transactions\n",
        "- Top Feature Importance which are important in model building are amt, Customer_age, category_food_dining, city_pop\n",
        "- New york,Texas ,Pennsylvania records major fraud cases.\n",
        "- Grocery pos, Shopping_net, misc_net categories have more likely to fraud\n",
        "-  Avg fraud transactions are more than the non fraudulent transactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4P6SampY2LW"
      },
      "source": [
        "# Cost-Benefit Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24i-rGcawH9s"
      },
      "outputs": [],
      "source": [
        "#Now basis on the questions splitting the train & test data basis on the target variable\n",
        "#Splitting the train data basis on target variable\n",
        "fraud_train_df=df_fraud_train[df_fraud_train['is_fraud']==1]\n",
        "non_fraud_train_df=df_fraud_train[df_fraud_train['is_fraud']==0]\n",
        "\n",
        "#Splitting the test data basis on target variable\n",
        "fraud_test_df=df_fraud_test[df_fraud_test['is_fraud']==1]\n",
        "non_fraud_test_df=df_fraud_test[df_fraud_test['is_fraud']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUYo5LAdHSz-"
      },
      "outputs": [],
      "source": [
        "df_fraud_train[df_fraud_train['is_fraud']==0]['amt'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_gOcJ6qZH-x"
      },
      "source": [
        "# Part I: Analyse the dataset and find the following figures:\n",
        "\n",
        "- Average number of transactions per month\n",
        "- Average number of fraudulent transactions per month\n",
        "- Average amount per fraudulent transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soAAz8NGZRBT"
      },
      "outputs": [],
      "source": [
        "#Average number of transactions per month on TRAINING DATA\n",
        "Avg_Transactions_Per_Month_Training = (len(df_fraud_train)/12)\n",
        "print('Avg No. Of Transactions per month on Training Data is : {}'.format(round(Avg_Transactions_Per_Month_Training),2))\n",
        "\n",
        "#Average number of transactions per month on TEST DATA\n",
        "Avg_Transactions_Per_Month_Test =  (len(df_fraud_test)/12)\n",
        "print('Avg No. Of Transactions per month on Test Data is : {}'.format(round(Avg_Transactions_Per_Month_Test),2))\n",
        "\n",
        "print('\\n')\n",
        "#Average number of fraudulent transactions per month on TRAINING DATA\n",
        "Avg_No_Fraud_Transaction_Per_month_Training = (len(fraud_train_df)/12)\n",
        "print('Avg No. Of Fraud Transactions per month on Training Data is : {}'.format(Avg_No_Fraud_Transaction_Per_month_Training))\n",
        "\n",
        "#Average number of fraudulent transactions per month on TEST DATA\n",
        "Avg_No_Fraud_Transaction_Per_month_Test = (len(fraud_test_df)/12)\n",
        "print('Avg No. Of Fraud Transactions per month on Test Data is : {}'.format(Avg_No_Fraud_Transaction_Per_month_Test))\n",
        "\n",
        "print('\\n')\n",
        "#Average amount per fraudulent transaction on TRAINING DATA\n",
        "Avg_Amt_Per_Fraud_Transaction_Training = fraud_train_df['amt'].mean()\n",
        "print('Avg Amt Per Fraud Transactions on Training is : {}'.format(round(Avg_Amt_Per_Fraud_Transaction_Training),2))\n",
        "\n",
        "#Average amount per fraudulent transaction on TEST DATA\n",
        "Avg_Amt_Per_Fraud_Transaction_Test = fraud_test_df['amt'].mean()\n",
        "print('Avg Amt Per Fraud Transactions on Test is : {}'.format(round(Avg_Amt_Per_Fraud_Transaction_Test),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxxi1gGZl2SV"
      },
      "source": [
        "# Part II: Compare the cost incurred per month by the bank before and after the model deployment:\n",
        "\n",
        "- Cost incurred per month before the model was deployed = Average amount per fraudulent transaction * Average number of fraudulent transactions per month\n",
        "\n",
        "- Cost incurred per month after the model is built and deployed: Use the test metric from the model evaluation part and the calculations performed in Part I to compute the values given below --\n",
        "\n",
        " 1) Let TF be the average number of transactions per month detected as fraudulent by the model and let the cost of providing customer executive support per fraudulent transaction detected by the model = $1.5\n",
        "\n",
        "   - Total cost of providing customer support per month for fraudulent   transactions detected by the model = 1.5 * TF.\n",
        "\n",
        " 2) Let FN be the average number of transactions per month that are fraudulent but not detected by the model\n",
        "\n",
        " - Cost incurred due to these fraudulent transactions left undetected by the model = Average amount per fraudulent transaction * FN\n",
        " - Therefore, the cost incurred per month after the model is built and deployed = 1.5*TF + Average amount per fraudulent transaction * FN\n",
        " - Final savings = Cost incurred before - Cost incurred after.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahd1LbxrltCK"
      },
      "outputs": [],
      "source": [
        "# Cost incurred per month before the model was deployed = Average amount per fraudulent transaction * Average number of fraudulent transactions per month\n",
        "# Cost incurred per month before the model was deployed for TRAINING data\n",
        "Avg_Amt_Per_Fraud_Transaction_Train = fraud_train_df['amt'].mean()\n",
        "Avg_No_Fraud_Transaction_Per_month_Train = (len(fraud_train_df)/12)\n",
        "\n",
        "Cost_incurred_per_month_before_the_model_was_deployed_Train = Avg_Amt_Per_Fraud_Transaction_Train * Avg_No_Fraud_Transaction_Per_month_Train\n",
        "print('Cost incurred per month before the model was deployed for Train data is : {}'.format(round(Cost_incurred_per_month_before_the_model_was_deployed_Train),2))\n",
        "\n",
        "print('\\n')\n",
        "# Cost incurred per month before the model was deployed for TEST data\n",
        "Avg_Amt_Per_Fraud_Transaction_Test = fraud_test_df['amt'].mean()\n",
        "Avg_No_Fraud_Transaction_Per_month_Test = (len(fraud_test_df)/12)\n",
        "\n",
        "Cost_incurred_per_month_before_the_model_was_deployed_Test = Avg_Amt_Per_Fraud_Transaction_Test * Avg_No_Fraud_Transaction_Per_month_Test\n",
        "print('Cost incurred per month before the model was deployed for Test data is : {}'.format(round(Cost_incurred_per_month_before_the_model_was_deployed_Test),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKSkxzVT4b22"
      },
      "source": [
        "#### Total cost of providing customer support per month for fraudulent transactions detected by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIPn7KaU3qeq"
      },
      "outputs": [],
      "source": [
        "# Average number of transactions per month detected as fraudulent by the model on TRAINING data\n",
        "TF_Train = (25364 + 540506)/12\n",
        "print('TF_Train is : {}'.format(TF_Train))\n",
        "\n",
        "# Cost of providing customer executive support per fraudulent transaction detected by the model = $1.5\n",
        "Total_cost_providing_custmr_support_per_mon_fraud_transactions_detected_by_the_model_Train = 1.5 * TF_Train\n",
        "print('Total cost of providing customer support per month for fraudulent transactions detected by the model on train data is : {}' .format(Total_cost_providing_custmr_support_per_mon_fraud_transactions_detected_by_the_model_Train))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "# Average number of transactions per month detected as fraudulent by the model on TEST data\n",
        "TF_Test = (2014 + 10945)/12\n",
        "print('TF_Test is : {}'.format(TF_Test))\n",
        "\n",
        "# Cost of providing customer executive support per fraudulent transaction detected by the model = $1.5\n",
        "Total_cost_providing_custmr_support_per_mon_fraud_transactions_detected_by_the_model_Test = 1.5 * TF_Test\n",
        "print('Total cost of providing customer support per month for fraudulent transactions detected by the model on test data is : {}' .format(Total_cost_providing_custmr_support_per_mon_fraud_transactions_detected_by_the_model_Test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWvu4HFn7lf7"
      },
      "source": [
        "### Cost incurred due to these fraudulent transactions left undetected by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTxIMnGY3qk3"
      },
      "outputs": [],
      "source": [
        "# Average number of transactions per month that are fraudulent but not detected by the model on TRAINING data\n",
        "FN_Train = 14796/12\n",
        "print('FN_Train is : {}'.format(FN_Train))\n",
        "\n",
        "# Cost incurred due to these fraudulent transactions left undetected by the model = Average amount per fraudulent transaction * FN\n",
        "Cost_fraud_trans_undetect_model_Train = Avg_Amt_Per_Fraud_Transaction_Training * FN_Train\n",
        "print('Cost incurred due to these fraudulent transactions left undetected by the model for Train data is : {}'.format(Cost_fraud_trans_undetect_model_Train))\n",
        "\n",
        "# the cost incurred per month after the model is built and deployed on TRAINING data\n",
        "Cost_per_mon_aftr_model_deployed_Train= Total_cost_providing_custmr_support_per_mon_fraud_transactions_detected_by_the_model_Train + Cost_fraud_trans_undetect_model_Train\n",
        "print('Cost incurred per month after the model is built and deployed on train data is : {}'.format(Cost_per_mon_aftr_model_deployed_Train))\n",
        "\n",
        "print('\\n')\n",
        "# Average number of transactions per month that are fraudulent but not detected by the model on TEST data\n",
        "FN_Test = 131/12\n",
        "print('FN_Test is : {}'.format(FN_Test))\n",
        "\n",
        "# Cost incurred due to these fraudulent transactions left undetected by the model on TEST data\n",
        "Cost_fraud_trans_undetect_model_Test = Avg_Amt_Per_Fraud_Transaction_Test * FN_Test\n",
        "print('Cost incurred due to these fraudulent transactions left undetected by the model for Test data is : {}'.format(Cost_fraud_trans_undetect_model_Test))\n",
        "\n",
        "# Cost incurred per month after the model is built and deployed on TEST data\n",
        "Cost_per_mon_aftr_model_deployed_Test = Total_cost_providing_custmr_support_per_mon_fraud_transactions_detected_by_the_model_Test + Cost_fraud_trans_undetect_model_Test\n",
        "print('Cost incurred per month after the model is built and deployed on test data is : {}'.format(Cost_per_mon_aftr_model_deployed_Test))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UAwTIcJFkfj"
      },
      "source": [
        "# Final savings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjkoNkHE3qqV"
      },
      "outputs": [],
      "source": [
        "# Final Savings on TRAINING data\n",
        "Final_Savings_Train = Cost_incurred_per_month_before_the_model_was_deployed_Train - Cost_per_mon_aftr_model_deployed_Train\n",
        "print('Final Savings on Train Data is : {}'.format(Final_Savings_Train))\n",
        "\n",
        "print('\\n')\n",
        "# Final Savings on TEST data\n",
        "Final_Savings_Test = Cost_incurred_per_month_before_the_model_was_deployed_Test - Cost_per_mon_aftr_model_deployed_Test\n",
        "print('Final Savings on Test Data is : {}'.format(Final_Savings_Test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g08btMClHn2n"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}